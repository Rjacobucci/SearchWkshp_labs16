---
title: "Exploratory Data Mining via Search Strategies Lab #5"
author: "Ross Jacobucci & Kevin J. Grimm"
fontsize: 8pt
output:
  beamer_presentation:
    colortheme: "wolverine"
    fonttheme: "structurebold"
---
## Outline

This presentation will go over the basics of using various multivariate procedures in R. These include:

1. Exploratory Factor Analysis \& PCA
2. Structural Equation Models
3. SEM Trees
4. Other Multivariate CART Models


## R Packages

EFA \& SEM
```{r,message=FALSE}
library(psych) # efa & miscellaneous tools
library(OpenMx) # SEM
library(lavaan) # SEM
```

SEM Trees
```{r,message=FALSE}
# how to install
#source('http://www.brandmaier.de/semtree/getsemtree.R')
library(semtree)
```

Other Longitudinal Trees

longRPart package. Not currently maintained on CRAN. Can be accessed from:
http://cran.r-project.org/src/contrib/Archive/longRPart/

First Download the 1.0 tar.gz
```{r,message=FALSE}
# longRPart is not on CRAN
# have to install from source
library(longRPart)
library(REEMtree)
```

## PCA and EFA

To do PCA:
prcomp() -- built-in

To do EFA:
factanal() -- builtin

fa() -- from **psych**; multiple upgrades

efaUnrotate() -- from **semTools**; can do FIML for missing data and WLSMV for categorical variables

GPA() -- from **GPArotation** -- one stop shop for factor rotations

**nFactors** package contains various functions for determining number of factors

CFA:
cfa() -- from **lavaan** package

General SEM:
**OpenMx** -- can do cfa,sem, mixtures, differential equations...Most general package

**lavaan** -- modeled after Mplus; can do maybe 80% of the things that Mplus can

We will be using the Holzinger Swineford dataset for all of the examples. Data from lavaan package

```{r,message=FALSE}
library(lavaan)
library(OpenMx)
# can't get OpenMx from CRAN
#source('http://openmx.psyc.virginia.edu/getOpenMx.R')
HS <- HolzingerSwineford1939
#summary(HS)
#str(HS)
```

# PCA, EFA, CFA

## PCA

```{r}
pca.out <- prcomp(HS[,7:15])
#quartz()
plot(pca.out)
```

Slightly ambiguous as to the number of components to retain, but we can see that the 3 components with eigenvalues above 1 (Kaiser rule). But in looking at the actual loadings, it almost looks like there is a general component, and maybe a couple specific components. 

## PCA continued
The psych package has a PCA function, principal(), which uses the same algorithm, but provides much more helpful output.

```{r,message=FALSE}
library(psych)
prin1 <- principal(HS[,7:15])
loadings(prin1)
```

## 2 Components
```{r}
prin2 <- principal(HS[,7:15],2)
loadings(prin2)
```
## 3 Components
```{r}
prin3 <- principal(HS[,7:15],3)
loadings(prin3)

```
## 4 Components
```{r}
prin4 <- principal(HS[,7:15],4)
loadings(prin4)
```

## PCA Continued
Note, PCA always extracts the same number of components as variables entered. But with principal() we have a choice of displaying a specific number of components.


In using PCA, 3 components seems to be a little bit cleaner,where we can see "clusters" in the loadings, than others in the factor structure. But still hazy. With 4 components, the last component is really only made up of 1 variable (loading > 0.9). 


One of the best tools that I know of to determine the number of components(PCA) or factors(EFA) is Horn's parallel analysis from the psych package.

Parallel analysis compares the actual eignevalues to the eigenvalues from a simulated dataset of random noise variables. We are looking for the number of eigenvalues above what would be expected by chance. This makes it look pretty clear, both 3 components and factors

## Parallel Analysis

Although called fa.parallel() it extracts both components and factors
```{r}
fa.parallel(HS[,7:15])
```

## Parallel Analysis With Items


Not Run
```{r,message=FALSE,eval=FALSE}
library(random.polychor.pa)
data(bfi)
bfi.data<-na.exclude(as.matrix(bfi[1:200, 1:5]))
out <- random.polychor.pa(nrep=3, data.matrix=bfi.data, q.eigen=.99)
```

If your variables have 1-5 or 6 categories, then steps need to be taken to change the estimation procedure for a number of methods we will be talking about, including parallel analysis.

## EFA

R has the built-in factanal() which gets the job done in most cases. Defaults to ML estimation and varimax(orthogonal rotation)
```{r}
fa.out <- factanal(HS[,7:15],3);loads <- fa.out$loadings
fa.out
```

## Rotation

```{r}
# cluster.plot(fa.out)
# extract loadings and feed to rotation program.
library(GPArotation)
gpa.out <- GPFoblq(loads) # oblique rotation
# new loading matrix
round(gpa.out$loadings,2)
# new factor correlations
gpa.out$Phi
```
Fairly clear factor structure. Not many cross-loadings.

Fancy way to plot results, from http://mindingthebrain.blogspot.com/2015/04/plotting-factor-analysis-results.html

## Factor Scores
Get factor scores:
```{r}
fa.out2 <- fa(HS[,7:15],scores="Bartlett")
fscor <- fa.out2$scores
head(fscor)
```
Not generally advisable to get factor scores as there are a number of inherent problems with them (Grice 2001), but the psych package's fa() has multiple options. see "scores=" option.


## Confirmatory Factor Analysis

What if we have an idea as to what the structure of a model is?


Great tutorial: http://lavaan.ugent.be/tutorial/tutorial.pdf


This means we can specify a simple structure to the model

```{r}
HS.model <- ' visual =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed =~ x7 + x8 + x9 '
fit <- cfa(HS.model, data=HolzingerSwineford1939)
#summary(fit, fit.measures=TRUE) too much output
coef(fit)

```


## Plot CFA Model

```{r,fig.height=4.5}
semPlot::semPaths(fit,what="est")
```

## Various SEM (CFA) Tools
Get Fit Measures
```{r,eval=FALSE}
fitMeasures(fit)
```
Modification Indices
```{r,eval=FALSE}
modindices(fit)
```
Note that lavaan actually has four different functions to run models:

lavaan() - requires you to specify the full model

cfa() - only have to specify part of the model, makes assumptions for CFA models

sem() - makes assumptions typical in more complex sem models

growth() - makes specifying LGM easy, only need intercept and slope specification

# Longitudinal Analyses

## WISC Data


For this demonstration, we are going to compare the different packages and functions in analyzing longitudinal WISC data. Data is WISC4VPE.DAT.

```{r}
wisc <- read.table("C:/Users/RJacobucci/Documents/GitHub/EDM_Labs/2015/wisc4vpe.dat")
names(wisc)<- c("V1","V2","V4","V6","P1","P2","P4", "P6", "Moeducat")
# note: V1 refers to verbal scores at grade 1, P is performance
```


## Visualization

To use many of the packages in R for longitudinal data (nlme), it is many times required to create a "long" data file, instead of the default wide.

How to do:
```{r}
# get rid of performance variables
wisc.verb <- wisc[,c(1:4,9)]

# create subset for plotting
ntot <- nrow(wisc.verb)    # total number of observations
wisc.verb.sel <- wisc.verb[sample(ntot, 30), ]

wisc.long <- reshape(wisc.verb, varying = c("V1", "V2", "V4", "V6"), v.names = "verbal",
                times = c(1, 2, 4, 6), direction = "long")

wisc.long.sel <- reshape(wisc.verb.sel, varying = c("V1", "V2", "V4", "V6"),
                         v.names = "verbal", times = c(1, 2, 4, 6), 
                         direction = "long")
head(wisc.long,3)
names(wisc.long)[2] <- "grade"
names(wisc.long.sel)[2] <- "grade"

```


## Trajectories

Lets take a look at what the trajectories are:

First using lattice package
```{r,message=FALSE,fig.height=5.5}
library(lattice)
xyplot(verbal ~ grade, groups = id, data = wisc.long, type = "o", col = "black", 
       xlab = "Grade of Testing", ylab = "Verbal[t]")
```

## Subset Trajectories
This is hard to see, better to use subset
```{r,fig.height=5.5}
xyplot(verbal ~ grade, groups = id, data = wisc.long.sel, type = "o", 
       col = "black", xlab = "Grade of Testing", ylab = "Verbal[t]")
# on average, scores went up over time
```
Thats a little better.

## GGplot2 

But, can we simultaneously view the trajectories over time while seeing the influence that Mother's education may have?

```{r,message=FALSE,fig.height=4.5}
library(ggplot2)
qplot(grade,verbal,group=id,data=wisc.long,alpha=I(1/2),colour=factor(Moeducat),
      geom = c("line","point"),xlab = "Grade of Testing", ylab = "Verbal[t]")
```
It seems pretty clear that mothers with higher levels of education have children that are consistently higher in verbal performance across time.

So now that we have an idea what will we find if we look at the relationship between mother's education and trajectory, lets test it with statistical models

# SEM Trees

## SEM Trees LGCM

### Note: SEM Trees can be used with any type of SEM that you can specify in lavaan or OpenMx

Our first example is going to be using a latent growth curve model (lgcm) as our outcome, and attempting to find subgroups based on mother's education and the performance scores

Previous demonstrations using SEM Trees have used OpenMx. In this case, we will use lavaan.

```{r,message=FALSE}
linearGCM <- '
    inter =~ 1*V1 + 1*V2 + 1*V4 + 1*V6
    slope =~ 1*V1 + 2*V2 + 4*V4 + 6*V6
    inter ~~ vari*inter; inter ~ meani*1;
    slope ~~ vars*slope; slope ~ means*1;
    inter ~~ cov*slope;
    V1 ~~ residual*V1; V1 ~ 0*1;
    V2 ~~ residual*V2; V2 ~ 0*1;
    V4 ~~ residual*V4; V4 ~ 0*1;
    V6 ~~ residual*V6; V6 ~ 0*1;'
run <- lavaan(linearGCM,wisc) # could also have used growth()
#summary(run)
coef(run)
```


## OpenMx
Same LGM as specified in lavaan

```{r,message=FALSE}
resVars <- mxPath( from=c("V1", "V2", "V4", "V6"), arrows=2,
                 free=TRUE, values = c(1,1,1,1),
                 labels=c("residual","residual","residual","residual") )
latVars<- mxPath( from=c("intercept","slope"), arrows=2, connect="unique.pairs",
                 free=TRUE, values=c(1,.4,1), labels=c("vari1","cov1","vars1") )
intLoads<- mxPath( from="intercept", to=c("V1", "V2", "V4", "V6"), arrows=1,
             free=FALSE, values=c(1,1,1,1) )
sloLoads<- mxPath( from="slope", to=c("V1", "V2", "V4", "V6"), arrows=1,
               free=FALSE, values=c(1,2,4,6) )
manMeans<- mxPath( from="one", to=c("V1", "V2", "V4", "V6"), arrows=1,
              free=FALSE, values=c(0,0,0,0) )
latMeans<- mxPath( from="one", to=c("intercept","slope"), arrows=1,
                free=TRUE,  values=c(0,1), labels=c("meani1","means1") )
dataRaw<- mxData( observed=wisc[,c(1:4,9)], type="raw" )
lgm.mod<- mxModel("LGM", type="RAM",
                manifestVars=c("V1", "V2", "V4", "V6"),
                latentVars=c("intercept","slope"),dataRaw,
                resVars, latVars, intLoads, sloLoads, manMeans, latMeans)
mod.run <- mxRun(lgm.mod);coef(mod.run)
```


## Run semtree()
Now use "run" with semtree()

### Note: semtree() works better with OpenMx at this time

Just used defaults
```{r,message=FALSE,fig.height=5}
mytree <- semtree(run,wisc[,c(1:4,9)]) # only moeducat as covariate
plot(mytree)
```

## Plot Trajectories

```{r,fig.height=5}
# create expected trajectories from parameters
expected.growth <- matrix(
    rep(t(parameters(mytree))[, "meani"], each=4) +
    rep(t(parameters(mytree))[, "means"], each=4)*c(1,2,4,6), nrow=2, byrow=T)
# plot expected trajectories for each leaf
plot(c(1,6), c(10,50), xlab="Grade", ylab="Verbal Score", type="n",main="SEM Trees LGCM")
lines(c(1,2,4,6), expected.growth[1,], col="red", type="b", lw=3)
lines(c(1,2,4,6), expected.growth[2,], col="blue", type="b", lw=3)
legend("bottomright", c("Mom Ed. = 0", "Mom Ed. = 1 or 2"),col=c("red","blue"), lw=3)
```


## SEM Trees Results Continued
We should get same results as in the left node of the tree by just subsetting the dataset based on Moeducat = 0

ould get same results as in the left node of the tree by just subsetting the dataset based on Moeducat = 0

```{r}
wisc.sub <- wisc[wisc$Moeducat == 0,]
run.sub <- lavaan(linearGCM,wisc.sub)
coef(run.sub)
```
Yup, everything checks out. This should make it clear that SEM Trees is really just subsetting the dataset into subgroups based on values of the covariates entered.

# SEM Trees Options

## Invariance of parameters
```{r,eval=FALSE}
model <- ' i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
           s =~ 0*t1 + l1*t2 + l2*t3 + 3*t4 '
fit <- growth(model, data=Demo.growth)
tree.inv = semtree(fit,Demo.growth[,1:5],invariance=c("l1","l2"))
```

This allows parameters to be freely estimated, but forces them to be the same in each group tested

## Additional Options

```{r,message=FALSE}
#?semtree.control
control <- semtree.control(method="fair",min.N=20,bonferroni=TRUE)
tree2 = semtree(mod.run,wisc[,c(1:4,9)],control=control)
#plot(tree2) Doesn't change results
```
I prefer to set the method="fair" as the default "naive" exhibits a preference for covariates with a large number of response options.
\
\
Additionally, I usually set the minimum number per node to be something greater than 20. Remember, each node is an actual SEM model.
\
\
Finally, if there are a large number of covariates, setting bonferroni=TRUE corrects for the number of comparisons. Usually doesn't make a difference

# SEM Trees 2nd Example


## Lavaan Syntax

For the 2nd example we will use a mediation model specified in lavaan 

```{r}
# latent variable definition           =~ is measured by
# regression                           ~ is regressed on
# (residual) (co)variance             ~~ is correlated (covariance) with
# intercept (mean)                  ~ 1 intercept   # same as regressed, but with 1
# mediation parameter definition   :=
```

## Mediation Example

Based on example from:
http://lavaan.ugent.be/tutorial/mediation.html

```{r}
set.seed(1234)
X <- rnorm(100)
class <- rbinom(100,1,0.5)
M <- class*0.5*X + rnorm(100)
Y <- class*0.7*M + rnorm(100)
Data <- data.frame(X = X, Y = Y, M = M,class=class)
model <- ' # direct effect
             Y ~ c*X
           # mediator
             M ~ a*X
             Y ~ b*M
           # indirect effect (a*b)
             ab := a*b
           # total effect
             total := c + (a*b)
         '
fit.med <- sem(model, data = Data)
```

## Visualize the Model

```{r}
semPlot::semPaths(fit.med)
```

## Mediation Continued
```{r}
summary(fit.med)
```

## SEM Trees and Mediation

```{r,message=FALSE}
tree.med <- semtree(fit.med,Data)
plot(tree.med)
```

## SEM Trees Conclusion

In the mediation example, SEM Trees correctly identified the groups with vastly different parameter estimates. 


The semtree package is still under development and bugs pop up occasionally. As an example, semtree currently doesn't work with R 3.3.1, thus why we had you install 3.2.5. 


### Important Reminder: the models you can run with SEM Trees is not limited to the type of models we presented. Any type of SEM can be used to search for groups.

# Mixed Effects Trees

## longRPart

Instead of running the models in a SEM framework, longRPart uses mixed-effects models. This works just as well, as many LGCM can be re-specified as mixed-effects models.

For this we will use the nlme package to run a linear mixed effects model

## nlme

```{r}
#Linear growth
mix1 <- lme(fixed = verbal ~ grade, random = ~ grade | id, 
            data = wisc.long, method="ML" )
summary(mix1) # get same estimates as in LGM, notice SD not VAR
```

Now that we see how we can specify growth curves as mixed-effects models, lets test out w/ longRPart and see if we get the same answer to SEM Trees

## longRPart

```{r,message=FALSE,results='hide'}
lcart.mod1 <- longRPart(verbal ~ grade,~ Moeducat, ~ 1 | id,wisc.long)
```
```{r}
summary(lcart.mod1)
```

## Plot Tree
```{r,fig.height=5}
lrpPlot(lcart.mod1)
lrpTreePlot(lcart.mod1,use.n=F)
```

Get almost identical results as from SEM Trees, but it looks as though longRPart allows more flexibility in the slopes between time points (grade).



## Additional Multivariate Trees


### REEMtree

Tree partitioning for longitudinal data where random effects exist. This doesn't really accomplish what we did previously with longRPart or SEM Trees. Interested, see the examples in following links.

http://pages.stern.nyu.edu/~jsimonof/REEMtree/

http://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests/

### mvpart

Like longRPart, also archived:

http://cran.r-project.org/src/contrib/Archive/mvpart/

If we treat the longitudinal data just as a multivariate outcome, we can accomplish a very similar process.

### mvtboost

Multivariate boosting

See for example code:
https://github.com/patr1ckm/mvtboost
